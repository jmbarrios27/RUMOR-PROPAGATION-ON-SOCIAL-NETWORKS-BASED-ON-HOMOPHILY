# -*- coding: utf-8 -*-
"""RUMOR_PROPAGATION_ON_SOCIAL_NETWORKS_BASED_ON_NETWORK_HOMOPHILY_(1) (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1czEtUUDj8NRtyP6O9cbNwrCAkk3hpnoJ

# **RUMOR PROPAGATION ON SOCIAL NETWORKS BASED ON NETWORK HOMOPHILY**

The work is based on finding homophily in social networks, to observe the behavior of spreading rumors within each of them. The Notebook is divided on three approaches:

**1-** Fake and True News Classification

**2-** Network Homophily

**3-** SIR Model (Susceptible, Recovered, Recovered)

## **DATASET**
For the development of the project, two data sets were used, which were collected for the study of “Detecting Fake News online Using N-Gram analysis and Machine Learning techniques” at the School of Computer Science at the University from Victoria in Canada. In it, information was collected through different news sources where the title of the news, the information it contains, the subject, and the date on which it was published are placed. It is structured in comma separated values ​​(CSV), one contains news with supposedly False or Fake news information, which for our study we will consider rumors and the other file of the same format that contains true information that we will consider as No rumor. Both files are structured in English.

## **A) FAKE AND TRUE NEWS CLASSIFICATION**`

---
"""

#Accesing to my Google Drive Account to take the Dataset
# Google file system
from google.colab import drive
drive.mount('/gdrive', force_remount=True)

#importing Libarries
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import nltk
import re
import networkx as nx
from wordcloud import WordCloud

"""## **FAKE NEWS**"""

#Fake News Dataset
ATT_FILE= "/gdrive/My Drive/TFM/Fake.csv"

fake = pd.read_csv(ATT_FILE,sep=',')
fake.head()

#Counting by Subjects 
for key,count in fake.subject.value_counts().iteritems():
    print(f"{key}:\t{count}")
    
#Getting Total Rows
print(f"Total Records:\t{fake.shape[0]}")

#SUBJECT VALUE COUNTPLOT
plt.figure(figsize=(8,5))
sns.countplot("subject", data=fake)
plt.title('SUBJECT VALUE COUNT')
plt.show()

#WordCloudg
import matplotlib.pylab as plt
allwords = ' '.join([fk for fk in fake['text']])
wordcloud = WordCloud(width=600, height=300, random_state=21, max_font_size=119).generate(allwords)
plt.figure(figsize=(8,6))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

"""### **REAL NEWS DATASET**"""

#REAL NEWS DATASET READING FROM GOOGLE DRIVE
att =  "/gdrive/My Drive/TFM/True.csv"

real = pd.read_csv(att, sep=',')
real.head()

#COUNTING THE NUMBER OF SUBECTS PER NEWS
real['subject'].value_counts()

#PLOT
sns.countplot('subject',data=real)
plt.title('Subject Value Count')

#First Creating list of index that do not have publication part
unknown_publishers = []
for index,row in enumerate(real.text.values):
    try:
        record = row.split(" -", maxsplit=1)
        #if no text part is present, following will give error
        record[1]
        #if len of piblication part is greater than 260
        #following will give error, ensuring no text having "-" in between is counted
        assert(len(record[0]) < 260)
    except:
        unknown_publishers.append(index)

#Thus we have list of indices where publisher is not mentioned
real.iloc[unknown_publishers].text
#tRYE

#Seperating Publication info, from actual text
publisher = []
tmp_text = []
for index,row in enumerate(real.text.values):
    if index in unknown_publishers:
        #Add unknown of publisher not mentioned
        tmp_text.append(row)
        
        publisher.append("Unknown")
        continue
    record = row.split(" -", maxsplit=1)
    publisher.append(record[0])
    tmp_text.append(record[1])

#Replace existing text column with new text
#add seperate column for publication info
real["publisher"] = publisher
real["text"] = tmp_text

#OBSERVERING LAST 5 ROWS, SHOWING THE PUBLISHER OF THE NEWS
real.tail()

#checking for rows with empty text like row:8970
[index for index,text in enumerate(real.text.values) if str(text).strip() == '']

#dropping this record because is empty
real = real.drop(8970, axis=0)

# checking for the same in fake news
empty_fake_index = [index for index,text in enumerate(fake.text.values) if str(text).strip() == '']
print(f"No of empty rows: {len(empty_fake_index)}")
fake.iloc[empty_fake_index].tail()

#Getting Total Rows
print(f"Total Records:\t{real.shape[0]}")

#Counting by Subjects 
for key,count in real.subject.value_counts().iteritems():
  print(f"{key}:\t{count}")

#Word Cloud for real news
import matplotlib.pylab as plt
allwords = ' '.join([rl for rl in real['text']])
wordcloud = WordCloud(width=600, height=300, random_state=21, max_font_size=119).generate(allwords)
plt.figure(figsize=(8,6))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

# Adding class Information
real["class"] = 1
fake["class"] = 0

# Subject is diffrent for real and fake thus dropping it
# Aldo dropping Date, title and Publication Info of real
real = real.drop(["subject", "date","title",  "publisher"], axis=1)
fake = fake.drop(["subject", "date", "title"], axis=1)

#Combining both into new dataframe
data = real.append(fake, ignore_index=True)

#observing the new column classifying the type of new (1 for real class and 0 for fake class)
data.head()

def rumorLabel(value):
  if value == 1:
    return 'No Rumor'
  else:
    return 'Rumor'

#NEW COLUMN WITH THE CLASS LABEL
data['ClassLabel'] = data['class'].apply(rumorLabel)
data

#countplot to compare it
sns.countplot(x='ClassLabel', data=data)
plt.title('No Rumor vs Rumor count')
plt.show()

plot_fake_true = data['ClassLabel'].value_counts().plot.pie(subplots=True, figsize=(7, 4),autopct='%1.1f%%')

#dropping the Classlabel Column
data = data.drop(columns=['ClassLabel'])

"""### **REMOVING STOPWORDS, STEMMING, TOKENIZING**"""

nltk.download('stopwords')
nltk.download('punkt')
y = data["class"].values
#Converting X to format acceptable by gensim, removing annd punctuation stopwords in the process
X = []
stop_words = set(nltk.corpus.stopwords.words("english"))
tokenizer = nltk.tokenize.RegexpTokenizer(r'\w+')
for par in data["text"].values:
    tmp = []
    sentences = nltk.sent_tokenize(par)
    for sent in sentences:
        sent = sent.lower()
        tokens = tokenizer.tokenize(sent)
        filtered_words = [w.strip() for w in tokens if w not in stop_words and len(w) > 1]
        tmp.extend(filtered_words)
    X.append(tmp)

del data

"""## **STOP-WORDS**"""

#PRINTING THE REMOVED STOPWORDS JUST TO CHECK
print(stop_words)

"""### **WORD2VEC**"""

import gensim

#Dimension of vectors we are generating
EMBEDDING_DIM = 100

#Creating Word Vectors by Word2Vec Method
w2v_model = gensim.models.Word2Vec(sentences=X, size=EMBEDDING_DIM, window=5, min_count=1)

from sklearn.manifold import TSNE
#Showing the plot for "trump" related words
def display_closestwords_tsnescatterplot(model, word):
    
    arr = np.empty((0,100), dtype='f')
    word_labels = [word]

    # get close words
    close_words = model.similar_by_word(word)
    
    # add the vector for each of the closest words to the array
    arr = np.append(arr, np.array([model[word]]), axis=0)
    for wrd_score in close_words:
        wrd_vector = model[wrd_score[0]]
        word_labels.append(wrd_score[0])
        arr = np.append(arr, np.array([wrd_vector]), axis=0)
        
    # find tsne coords for 2 dimensions
    tsne = TSNE(n_components=2, random_state=0)
    np.set_printoptions(suppress=True)
    Y = tsne.fit_transform(arr)

    x_coords = Y[:, 0]
    y_coords = Y[:, 1]
    # display scatter plot
    plt.scatter(x_coords, y_coords)

    for label, x, y in zip(word_labels, x_coords, y_coords):
        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')
    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)
    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)
    plt.title('Word2Vec Trump related Words')
    plt.show()

display_closestwords_tsnescatterplot(w2v_model, 'trump')

w2v_model.wv.most_similar("iran")

w2v_model.wv.most_similar("fbi")

# Tokenizing Text -> Repsesenting each word by a number
# Mapping of orginal word to number is preserved in word_index property of tokenizer
from keras.preprocessing.text import Tokenizer
#Tokenized applies basic processing like changing it yo lower case, explicitely setting that as False
tokenizer = Tokenizer()
tokenizer.fit_on_texts(X)

X = tokenizer.texts_to_sequences(X)

# lets check the first 10 words of first news
#every word has been represented with a number
X[0][:10]

#Lets check few word to numerical replesentation
#Mapping is preserved in dictionary -> word_index property of instance
word_index = tokenizer.word_index
for word, num in word_index.items():
    print(f"{word} -> {num}")
    if num == 21:
        break

nos = np.array([len(x) for x in X])
len(nos[nos  <10000000])
# Out of 48k news, 42k have less than 2000 words

#Lets keep all news to 500, add padding to news with less than 500 words and truncating long ones
from keras.preprocessing.sequence import pad_sequences
maxlen = 4000

#Making all news of size maxlen defined above
X = pad_sequences(X, maxlen=maxlen)

print(len(X))

# Adding 1 because of reserved 0 index
# Embedding Layer creates one more vector for "UNKNOWN" words, or padded words (0s). This Vector is filled with zeros.
# Thus our vocab size inceeases by 1
vocab_size = len(tokenizer.word_index) + 1
vocab_size

# Function to create weight matrix from word2vec gensim model
def get_weight_matrix(model, vocab):
    # total vocabulary size plus 0 for unknown words
    vocab_size = len(vocab) + 1
    # define weight matrix dimensions with all 0
    weight_matrix = np.zeros((vocab_size, EMBEDDING_DIM))
    # step vocab, store vectors using the Tokenizer's integer mapping
    for word, i in vocab.items():
        weight_matrix[i] = model[word]
    return weight_matrix

#Getting embedding vectors from word2vec and usings it as weights of non-trainable keras embedding layer
embedding_vectors = get_weight_matrix(w2v_model, word_index)
print(embedding_vectors)

"""### **NEURAL NETWORK FOR RUMOR CLASSIFICATION**"""

#Defining Neural Network
from tensorflow.keras.optimizers import SGD
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Embedding, LSTM,Dense
import random
random.seed(30)
model = Sequential()
#Non-trainable embeddidng layer
model.add(Embedding(vocab_size, output_dim=EMBEDDING_DIM, weights=[embedding_vectors], input_length=maxlen, trainable=False))
#LSTM 
model.add(LSTM(units=128))
model.add(Dense(1, activation='tanh'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])

model.summary()

from sklearn.model_selection import train_test_split
#Train test split
X_train, X_test, y_train, y_test = train_test_split(X, y)

#model fit
model.fit(X_train, y_train, validation_split=0.3, epochs=6)

#Prediction is in probability of news being real, so converting into classes
# Class 0 (Fake) if predicted prob < 0.5, else class 1 (Real)
y_pred = (model.predict(X_test) >= 0.5).astype("int")

#Accuracy of the model
from sklearn.metrics import accuracy_score
print('The Accuracy of the Neural Network is: ',accuracy_score(y_test, y_pred))

#Checking other metrics like, precision, recall, f1-score
from sklearn.metrics import classification_report
print(classification_report(y_test, y_pred))

from sklearn.metrics import confusion_matrix
confusion = confusion_matrix(y_test,y_pred)
ax= plt.subplot()
sns.heatmap(confusion, annot=True, ax = ax,fmt=".0f"); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Rumor', 'No Rumor']); ax.yaxis.set_ticklabels(['Rumor', 'No Rumor'])

"""## **B) NETWORK HOMOPHILY**


---
"""

#Fake News Dataset
ATT_FILE= "/gdrive/My Drive/TFM/Fake.csv"

fake = pd.read_csv(ATT_FILE,sep=',')
fake.head()

def cleanText(text):
    text = re.sub(r'@[A-Za-z]','',text)#Removes @mentions
    text = re.sub(r'#','',text)#Removing the Hashtag
    text = re.sub(r'RT[\s]','',text)#Removing RT
    text = re.sub(r'_','',text)#Removing RT
    #text. re.sub(r'https?:\/\/\S+')#Remove the Hyper Link, in case of having one, in this case les comment it because hyper links are not in this case
    return text

"""### **EXAMPLES OF TWO GROUPTS WITHIN A NETWORK**"""

# Author: Aric Hagberg (hagberg@lanl.gov)
import matplotlib.pyplot as plt
import networkx as nx

G = nx.cubical_graph()
pos = nx.spring_layout(G)  # positions for all nodes

# nodes
nx.draw_networkx_nodes(G, pos,
                       nodelist=[0, 1, 2, 3],
                       node_color='r',
                       node_size=1000,
                       alpha=0.8)
nx.draw_networkx_nodes(G, pos,
                       nodelist=[4, 5, 6, 7],
                       node_color='b',
                       node_size=1000,
                       alpha=0.8)

# edges
nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)
nx.draw_networkx_edges(G, pos,
                       edgelist=[(0, 1), (1, 2), (2, 3), (3, 0)],
                       width=8, alpha=0.5, edge_color='r')
nx.draw_networkx_edges(G, pos,
                       edgelist=[(4, 5), (5, 6), (6, 7), (7, 4)],
                       width=8, alpha=0.5, edge_color='b')


# some math labels
labels = {}
labels[0] = r'$Gov$'
labels[1] = r'$Gov$'
labels[2] = r'$Gov$'
labels[3] = r'$Gov$'
labels[4] = r'$Int$'
labels[5] = r'$Int$'
labels[6] = r'$Int$'
labels[7] = r'$Int$'
nx.draw_networkx_labels(G, pos, labels, font_size=14)

plt.axis('off')
plt.title('Network Homophily')
plt.show()

sns.countplot(x='subject',data=fake)

# Commented out IPython magic to ensure Python compatibility.
import copy
import collections
import networkx as nx
import random
import operator
import numpy as np

import matplotlib.pyplot  as plt
from numpy.random import choice
# %matplotlib inline

g = nx.Graph()
g.add_nodes_from(['News', 'Politics','Government News','Left-News','US-News','Middle-east'])
print(g.nodes())

g.add_edges_from([('News','Politics'), ('News','Government News'),('News','Left-News'),('News','US-News'),('News','Middle-east'),
                  ('Politics','News'), ('Politics','Government News'),('Politics','Left-News'),('Politics','US-News'),('Politics','Middle-east'),
                  ('Government News','News'), ('Government News','Politics'),('Government News','Left-News'),('Government News','US-News'),('Government News','Middle-east'),
                  ('Left-News','News'), ('Left-News','Politics'),('Left-News','Government News'),('Left-News','US-News'),('Left-News','Middle-east'),
                  ('US-News','News'), ('US-News','Politics'),('US-News','Government News'),('US-News','Left-News'),('US-News','Middle-east'),
                  ('Middle-east','News'), ('Middle-east','Politics'),('Middle-east','Government News'),('Middle-east','Left-News'),('Middle-east','US-News')])

color_map = []
for node in g:
    if node =='News':
        color_map.append('pink')
    elif node =='Politics': 
        color_map.append('green')
    elif node =='Government News': 
        color_map.append('red')
    elif node =='Left-News': 
        color_map.append('yellow') 
    elif node =='US-News': 
        color_map.append('orange') 
    elif node =='Middle-east': 
        color_map.append('lightblue')

nx.draw(g, node_color=color_map, with_labels=True, node_size=500,edge_color='black',arrowsize=60
        )

G = nx.erdos_renyi_graph(20, 0.1)

"""# **POLITICAL BIAS BASED ON SHARED NEWS**"""

#Creating dataset
posts = fake[fake['subject']=='politics']

#counting the number of rows
posts['subject'].value_counts()

#Information of the dataset
fake.info()

#Description of the dataset
fake.describe()

"""# **PRE-PROCESSING OF THE DATASET**"""

#New dataset based on posts
data = posts

#Creando funcion para convertir politics en 
def turning_one(value):
  if value == 'politics':
    return 1

#Applying function for the class
data['class'] = data['subject'].apply(turning_one)

#Dataset without any cleaning
sin_limpiar = data

#WordCloudg
import matplotlib.pylab as plt
allwords = ' '.join([fk for fk in data['text']])
wordcloud = WordCloud(width=600, height=300, random_state=21, max_font_size=119).generate(allwords)
plt.figure(figsize=(8,6))
plt.imshow(wordcloud)
plt.axis('off')
plt.show()

#Dropping columns of tittles and dates
data = data.drop(columns=['title','subject','text'])

#Checking the info
print(data.info())

data['date'].describe()

#Grouping by date
hola = data.groupby('date').sum()

#Converting into a New Dataframe
hola= pd.DataFrame(hola)

import plotly.graph_objects as go
import plotly 
from plotly import tools
from plotly.offline import init_notebook_mode, plot, iplot
all_words = sin_limpiar['text'].str.split(expand=True).unstack().value_counts()
datoss= [go.Bar(
            x = all_words.index.values[2:50],
            y = all_words.values[2:50],
            marker= dict(colorscale='Jet',
                         color = all_words.values[2:100]
                        ),
            text='Word counts'
    )]

layout = go.Layout(
    title='Top 50 (without cleaning) most frequent words in politics rumors texts '
)

fig = go.Figure(data=datoss, layout=layout)

iplot(fig, filename='basic-bar')

#Functiong for Data cleaning and Stemming
from bs4 import BeautifulSoup
import re
import nltk
nltk.download('stopwords')
nltk.download('punkt')
def prep(review):
    
    # Remove HTML tags.
    review = BeautifulSoup(review,'html.parser').get_text()
    
    # Remove non-letters
    review = re.sub("[^a-zA-Z]", " ", review)
    
    # Lower case
    review = review.lower()
    
    # Tokenize to each word.
    token = nltk.word_tokenize(review)
    
    # Stemming
    review = [nltk.stem.SnowballStemmer('english').stem(w) for w in token]
    
    # Join the words back into one string separated by space, and return the result.
    return " ".join(review)

# test whether the function successfully preprocessed.
sin_limpiar['text'].iloc[:2].apply(prep).iloc[0]

# If there is no problem at the previous cell, let's apply to all the rows.
sin_limpiar['clean'] = sin_limpiar['text'].apply(prep)

print('shape is:',sin_limpiar.shape)

#Creating a cleaned dataset with "class"
limpio = sin_limpiar[['clean','class']]

"""## **SENTIMENT ANALYSIS WITH TF-IDF FIRST APPROACH**"""

import re
from textblob import TextBlob

#Create a Function to clean text, La r es para decirle a python que son caracters raw
def cleanText(text):
    text = re.sub(r'@[A-Za-z0-9]','',text)#Removes @mentions
    text = re.sub(r'#','',text)#Removing the Hashtag
    text = re.sub(r'RT[\s]','',text)#Removing RT
    text = re.sub(r'_','',text)#Removing RT
    #text. re.sub(r'https?:\/\/\S+',text)#Remove the Hyper Link, in case of having one, in this case les comment it because hyper links are not in this case
    return text

#Clean Text function was applied to the Tweet Column valunes to get natural words. This user apparently have some typos, but it doesn`t matter 
limpio['new_clean']=limpio['clean'].apply(cleanText)
limpio['new_clean']

#Create a function for subectivity
def getSubjectivity(text):
  return TextBlob(text).sentiment.subjectivity

#Create a Function to get polarity despite that we have a column with that information
def getPolarity(text):
  return TextBlob(text).sentiment.polarity

#Create two new columns with that information
limpio['Subjectivity'] = limpio['new_clean'].apply(getSubjectivity)
limpio['Polarity'] = limpio['new_clean'].apply(getPolarity)
#Show the last 10
limpio.tail()

limpio['Polarity'].describe()

#Create a function to compute, the negative, neutral and positive analysis
def getAnalysis(score):
  if score >= 0:
    return  1
  elif score <0:
    return 0
  
  
#Create a new Column based on this
limpio['Sentiment']= limpio['Polarity'].apply(getAnalysis)

#Counting Sentiment Values for the the dataset with cleaned text
limpio['Sentiment'].value_counts()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import matplotlib.cm as cm

from sklearn.cluster import MiniBatchKMeans
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

tfidf = TfidfVectorizer(
    min_df = 5,
    max_df = 0.95,
    max_features = 8000,
    stop_words = 'english'
)
tfidf.fit(limpio.clean)
text = tfidf.transform(limpio.clean)

def find_optimal_clusters(data, max_k):
    iters = range(2, max_k+1, 2)
    
    sse = []
    for k in iters:
        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=9).fit(text).inertia_)
        print('Fit {} clusters'.format(k))
        
    f, ax = plt.subplots(1, 1)
    ax.plot(iters, sse, marker='o')
    ax.set_xlabel('Cluster Centers')
    ax.set_xticks(iters)
    ax.set_xticklabels(iters)
    ax.set_ylabel('SSE')
    ax.set_title('SSE by Cluster Center Plot')
    
find_optimal_clusters(text, 9)

clusters = MiniBatchKMeans(n_clusters=4, init_size=1024, batch_size=2048, random_state=9).fit_predict(text)

def plot_tsne_pca(limpio, labels):
    max_label = max(labels)
    max_items = np.random.choice(range(limpio.shape[0]), size=3000, replace=False)
    
    pca = PCA(n_components=2).fit_transform(limpio[max_items,:].todense())
    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(limpio[max_items,:].todense()))
    
    
    idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)
    label_subset = labels[max_items]
    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]
    
    f, ax = plt.subplots(1, 2, figsize=(14, 6))
    
    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)
    ax[0].set_title('PCA Cluster Plot')
    
    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)
    ax[1].set_title('TSNE Cluster Plot')
    
plot_tsne_pca(text, clusters)

def get_top_keywords(data, clusters, labels, n_terms):
    df = pd.DataFrame(data.todense()).groupby(clusters).mean()
    
    for i,r in df.iterrows():
        print('\nCluster {}'.format(i))
        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))
            
get_top_keywords(text, clusters, tfidf.get_feature_names(), 20)

"""# **SENTIMENT WITH TF-IDF SECOND APPROACH**"""

from bs4 import BeautifulSoup
import re
import nltk

limpio.head()

#Creating a new dataframe with the same values of the cleaned dataset just for backup, and use it afterwards
X_train = limpio

#Renaming columns
X_train.columns = ['review','class','new_clean','Subjectivity','Polarity','Sentiment']

def prep(review):
    
    # Remove HTML tags.
    review = BeautifulSoup(review,'html.parser').get_text()
    
    # Remove non-letters
    review = re.sub("[^a-zA-Z]", " ", review)
    
    # Lower case
    review = review.lower()
    
    # Tokenize to each word.
    token = nltk.word_tokenize(review)
    
    # Stemming
    review = [nltk.stem.SnowballStemmer('english').stem(w) for w in token]
    
    # Join the words back into one string separated by space, and return the result.
    return " ".join(review)

# test whether the function successfully preprocessed.
X_train['review'].iloc[:2].apply(prep).iloc[0]

# If there is no problem at the previous cell, let's apply to all the rows.
X_train['clean'] = X_train['review'].apply(prep)

X_train['clean'].iloc[3]

print('Training dim:',X_train.shape)

"""## **TF-IDF WITH X_TRAIN CLEANED DATASET**"""

#Fitting TF-IDF
tv = TfidfVectorizer(
                    ngram_range = (1,3),
                    sublinear_tf = True,
                    max_features = 40000)

train_tv = tv.fit_transform(X_train['clean'])

# Create the list of vocabulary used for the vectorizer.

vocab = tv.get_feature_names()
print(vocab[:5])

print("Vocabulary length:", len(vocab))

dist = np.sum(train_tv, axis=0)
checking = pd.DataFrame(dist,columns = vocab)

checking['twitter']

def find_optimal_clusters(data, max_k):
    iters = range(2, max_k+1, 2)
    
    sse = []
    for k in iters:
        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=7).fit(train_tv).inertia_)
        print('Fit {} clusters'.format(k))
        
    f, ax = plt.subplots(1, 1)
    ax.plot(iters, sse, marker='o')
    ax.set_xlabel('Cluster Centers')
    ax.set_xticks(iters)
    ax.set_xticklabels(iters)
    ax.set_ylabel('SSE')
    ax.set_title('SSE by Cluster Center Plot')
    
find_optimal_clusters(train_tv, 7)

clusters_2 = MiniBatchKMeans(n_clusters=3, init_size=1024, batch_size=2048, random_state=9).fit_predict(train_tv)

def plot_tsne_pca(X_train, labels):
    max_label = max(labels)
    max_items = np.random.choice(range(X_train.shape[0]), size=3000, replace=False)
    
    pca = PCA(n_components=3).fit_transform(X_train[max_items,:].todense())
    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(X_train[max_items,:].todense()))
    
    
    idx = np.random.choice(range(pca.shape[0]), size=300, replace=False)
    label_subset = labels[max_items]
    label_subset = [cm.hsv(i/max_label) for i in label_subset[idx]]
    
    f, ax = plt.subplots(1, 2, figsize=(14, 6))
    
    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)
    ax[0].set_title('PCA Cluster Plot')
    
    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)
    ax[1].set_title('TSNE Cluster Plot')
    
plot_tsne_pca(train_tv, clusters_2)

def get_top_keywords(data, clusters, labels, n_terms):
    df = pd.DataFrame(data.todense()).groupby(clusters_2).mean()
    
    for i,r in df.iterrows():
        print('\nCluster {}'.format(i))
        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))
            
get_top_keywords(train_tv, clusters_2, tv.get_feature_names(), 50)

holaa = get_top_keywords(train_tv, clusters_2, tv.get_feature_names(), 100)

#Creating dataframe for words contained in the clusters
sesgo = {
    'Cluster2':['ay','campaign','novemb','pretti','guy','will','januari','all','up','but','trump support','our','do','as','if','speech','peopl','how','they','not','hillari clinton','can','so','american','when','love','wow','cnn','https youtu be','https youtu','america','protest','youtu be','youtu','has','from','like','get','ralli','have','co','https co','who','of the','by','support','just','presid','out','are','what','she','about','donald trump','we','was','at','her','here','obama','great','this is','donald','be','his','clinton','he','that','video','with','it','for','youtub','you','on','https www youtub','www','https www','www youtub','www youtub com','youtub com','youtub com watch','com watch','in','hillari','watch','is','this','and','pic','pic twitter','pic twitter com','of','https','twitter','twitter com','trump','to','com','the'],
    'Cluster1':['here','hillari','nation','donald trump','also','want','white','america','donald','news','call','democrat','becaus','over','new','re','them','which','at the','how','get','support','other','make','him','to be','for the','time','report','no','go','there','do','after','on the','just','had','obama','up','american','can','so','were','like','state','if','year','say','been','would','or','more','all','when','our','out','one','what','to the','will','peopl','her','their','but','she','about','an','presid','has','from','you','in the','by','we','at','not','they','have','are','said','who','his','this','of the','as','be','trump','with','was','he','it','for','is','on','that','in','and','of','to','the']
}

sesgo = pd.DataFrame(sesgo)
sesgo['Cluster1'].value_counts()

tv.get_feature_names

X_train = X_train.drop(columns=['new_clean','Subjectivity','Polarity'])
X_train.head()

X_train = X_train.drop(columns=['class'])

"""## **MODELING**"""

from sklearn.svm import LinearSVC
from sklearn.naive_bayes import BernoulliNB
from sklearn.model_selection import GridSearchCV, StratifiedKFold, learning_curve
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import VotingClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.model_selection import KFold

limpio.info()

kfold = StratifiedKFold( n_splits = 5, random_state = 2018 )

"""## **LOGISTIC REGRESSION**"""

lr = LogisticRegression(random_state = 2018)

lr2_param = {
    'penalty':['l2'],
    'dual':[False],
    'C':[6],
    'class_weight':[{1:1}],
    }

lr_CV = GridSearchCV(lr, param_grid = [lr2_param], cv = kfold, scoring = 'roc_auc', n_jobs = 1, verbose = 1)
lr_CV.fit(train_tv, X_train['Sentiment'])
print(lr_CV.best_params_)
logi_best = lr_CV.best_estimator_

print(lr_CV.best_score_)

"""## **Investigating Model Coefficients**"""

# Extract the coefficients from the best model Logistic Regression and sort them by index.
coefficients = logi_best.coef_
index = coefficients.argsort()

# Extract the feature names.
feature_names = np.array(tv.get_feature_names())

# From the smallest to largest.
feature_names[index][0][:30]

# feature names: Smallest 30 + largest 30.
feature_names_comb = list(feature_names[index][0][:30]) + list(feature_names[index][0][-31::1])

# coefficients magnitude: Smallest 30 + largest 30.
index_comb = list(coefficients[0][index[0][:30]]) + list(coefficients[0][index[0][-31::1]])

# Make sure the x-axis be the number from 0 to the length of the features selected not the feature names.
# Once the bar is plotted, the features are placed as ticks.
plt.figure(figsize=(18,8))
barlist = plt.bar(list(i for i in range(61)), index_comb)
plt.xticks(list(i for i in range(61)),feature_names_comb,rotation=75,size=15)
plt.ylabel('Coefficient magnitude',size=20)
plt.xlabel('Features',size=20)

# color the first smallest 30 bars red
for i in range(30):
    barlist[i].set_color('red')

plt.show()

sns.countplot(x='Sentiment',data=X_train)
plt.title('Negative Sentiment vs Positive Sentiment')

#observing the type for clusters
clusters

"""# **UNSUPERVISED LEARNING FOR POLITICAL BIAS**

### **K-MEANS**
"""

#Importing librarries for ML
from sklearn.cluster import KMeans
from sklearn.metrics import pairwise_distances_argmin_min
from sklearn.neighbors import KNeighborsClassifier

#checking the type of text
text
#Have to convert it into an array

#Creating a new Sparse Matrix based on text
R = text

# let´s look for the clusters minimizing the WCSS
wcss = []
for i in range(1,5):
  kmeans=KMeans(n_clusters=i,max_iter=300)
  kmeans.fit(R)
  wcss.append(kmeans.inertia_)

#Plot for Jambu Elbow
plt.plot(range(1,5),wcss,'-o')
plt.title('Jambu Elbow')
plt.xlabel('Cluster Numbers')
plt.ylabel('WCSS')

plt.show()

#Plot for Elbow
Nc = range(1, 5)
kmeans = [KMeans(n_clusters=i) for i in Nc]
kmeans
score = [kmeans[i].fit(R).score(R) for i in range(len(kmeans))]
score
plt.plot(Nc,score,'-*')
plt.xlabel('Number of Clusters')
plt.ylabel('Score')
plt.title('Elbow Curve')
plt.figure(figsize=(8,6))
plt.show()

#fitting the K-means Algorithm
clustering = KMeans(n_clusters=3,max_iter=3000)
clustering.fit(R)

#Añadiendo Columna para agrear los dos clusters encontrados
limpio['Kmeans_cluster'] = clustering.labels_
limpio.head(5)

limpio.isna().sum().sum()

limpio.shape

final_clean_dataset = limpio

#Dataframe with only the clusters
kmeans_values = final_clean_dataset['Kmeans_cluster']
kmeans_values = pd.DataFrame(kmeans_values)
kmeans_values = kmeans_values.reset_index(drop=True)

#equating X with R for TSNE graph
X = R

#Creando Dataset con los componententes 
from sklearn.decomposition import PCA
from sklearn.decomposition import TruncatedSVD
from scipy.sparse import random as sparse_random
from sklearn.decomposition import SparsePCA
from sklearn.random_projection import sparse_random_matrix
pca_a = PCA(n_components=3) #grafico 2d
pca_review = pca_a.fit_transform(R.toarray())

#Dataframe with Components and K_means Clusters
pca_review_df = pd.DataFrame(data= pca_review, columns= ['Component1','Component2','Component3'])
pca_name_review = pd.concat([pca_review_df, kmeans_values[['Kmeans_cluster']]],axis=1)

pca_name_review['Kmeans_cluster'].value_counts()

#Shape of the dataframe
pca_name_review.shape

#PCA plot
fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(1,1,1)
ax.set_xlabel('Component1',fontsize=15)
ax.set_ylabel('Component2',fontsize=15)
ax.set_title('PCA',fontsize ='20')


color_theme = np.array(['blue','purple','red','green'])
ax.scatter(x=pca_name_review.Component2, y=pca_name_review.Component3,c=color_theme[pca_name_review.Kmeans_cluster],s=25)

plt.show()

import numpy as np
from sklearn.manifold import TSNE
X = R
import numpy as np
from sklearn.manifold import TSNE
X_embedded = TSNE(n_components=3).fit_transform(X)
X_embedded.shape

#Creating a dataframe for the TSN
tsn_review_df = pd.DataFrame(data= X_embedded, columns= ['Component1','Component2','Component3'])
tsn_name_review = pd.concat([tsn_review_df, kmeans_values[['Kmeans_cluster']]],axis=1)
tsn_name_review

#TSNE plot
fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(1,1,1)
ax.set_title('TSN Cluster Plot',fontsize ='20')
color_theme = np.array(['blue','purple','red','green'])
ax.scatter(x=tsn_name_review.Component1, y=tsn_name_review.Component2,c=color_theme[pca_name_review.Kmeans_cluster],s=25,alpha=1)

plt.show()

pca_name_review['pca-one'] = pca_review[:,0]
pca_name_review['pca-two'] = pca_review[:,1] 
pca_name_review['pca-three'] = pca_review[:,2]
print('Explained variation per principal component: {}'.format(pca_a.explained_variance_ratio_))

# For reproducability of the results
np.random.seed(42)
rndperm = np.random.permutation(pca_name_review.shape[0])

#Creating Herarchical dataset
hierarchical = pca_name_review.drop(columns=['pca-one','pca-two','pca-three'])
hierarchical.columns = ['value_one','value_two','value_three','target']

plt.figure(figsize=(10,8))
plt.title('Scatterplot Clusters Political Bias')
sns.scatterplot(
    x="pca-one", y="pca-two",
    hue='Kmeans_cluster',
    palette=sns.color_palette('tab10', 3),
    data=pca_name_review.loc[rndperm,:],
    legend="full",
    alpha=0.3
)

ax = plt.figure(figsize=(12,10)).gca(projection='3d')
ax.scatter(
    xs=pca_name_review.loc[rndperm,:]["pca-one"], 
    ys=pca_name_review.loc[rndperm,:]["pca-two"], 
    zs=pca_name_review.loc[rndperm,:]["pca-three"], 
    c=pca_name_review.loc[rndperm,:]["Kmeans_cluster"], 
    cmap='rainbow'
)
ax.set_xlabel('pca-one')
ax.set_ylabel('pca-two')
ax.set_zlabel('pca-three')
plt.title('PCA Clusters Political Bias')
plt.show()

"""#### **ACCURACY FOR K-MEANS**"""

k_means_accuracy = pca_name_review
k_means_accuracy = k_means_accuracy.drop(columns=['pca-one','pca-two','pca-three'])
from sklearn.model_selection import train_test_split

x_kmeans = k_means_accuracy.iloc[:,:3]

#Target Varialbe
y_kmeans =k_means_accuracy.iloc[:,-1]
k_means_predicted_labels = clustering.labels_

#Confusion Matrix
cmk= confusion_matrix(y_kmeans,k_means_predicted_labels)

ax= plt.subplot()
sns.heatmap(cmk, annot=True, ax = ax,fmt=".0f"); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Democrats', 'Neutral','Republicans']); ax.yaxis.set_ticklabels(['Democrats', 'Neutral','Republicans'])

accuracy_score(y_kmeans,k_means_predicted_labels)

print(classification_report(y_kmeans, k_means_predicted_labels))

"""## **HIERARCHICHAL CLUSTERING**"""

from sklearn.cluster import AgglomerativeClustering
import scipy.cluster.hierarchy as sch

X_hierarchichal = hierarchical.iloc[:,:3]

dendrogram = sch.dendrogram(sch.linkage(X_hierarchichal, method='ward'))

model_h = AgglomerativeClustering(n_clusters=4, affinity='euclidean', linkage='ward')
model_h.fit(X_hierarchichal)
labels = model_h.labels_

y_hiera = hierarchical.iloc[:,-1]
#Confusion Matrix
cmh= confusion_matrix(y_hiera,labels)

ax= plt.subplot()
sns.heatmap(cmh, annot=True, ax = ax,fmt=".0f"); #annot=True to annotate cells

# labels, title and ticks
ax.set_xlabel('Predicted labels');ax.set_ylabel('True labels'); 
ax.set_title('Confusion Matrix'); 
ax.xaxis.set_ticklabels(['Democrats', 'Neutral','Republicans','not identified']); ax.yaxis.set_ticklabels(['Democrats', 'Neutral','Republicans','not identified'])

accuracy_score(y_hiera,labels)

print(classification_report(y_hiera, labels))

"""## **CHECKING THE RELEVANT WORDS FOR EACH CLUSTER**"""

id_cluster_0 = limpio[limpio['Kmeans_cluster']==0]
id_cluster_1 = limpio[limpio['Kmeans_cluster']==1]
id_cluster_2 = limpio[limpio['Kmeans_cluster']==2]

"""## **TF-IDF FOR DEMOCRATS, NEUTRAL AND REPUBLICANS**"""

cluster0_tf = TfidfVectorizer(
                    ngram_range = (1,3),
                    sublinear_tf = True,
                    max_features = 8000)

train_cluster0_tf = cluster0_tf.fit_transform(limpio['clean'])

vocab_cluster0 = cluster0_tf.get_feature_names()
print(vocab_cluster0[:20])

dist_clust0 = np.sum(train_cluster0_tf, axis=0)
checking_clust0 = pd.DataFrame(dist_clust0,columns = vocab_cluster0)

checking_clust0

# def find_optimal_clusters(data, max_k):
#     iters = range(2, max_k+1, 2)
    
#     sse = []
#     for k in iters:
#         sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=7).fit(train_cluster0_tf).inertia_)
#         print('Fit {} clusters'.format(k))
        
#     f, ax = plt.subplots(1, 1)
#     ax.plot(iters, sse, marker='o')
#     ax.set_xlabel('Cluster Centers')
#     ax.set_xticks(iters)
#     ax.set_xticklabels(iters)
#     ax.set_ylabel('SSE')
#     ax.set_title('SSE by Cluster Center Plot')
    
# find_optimal_clusters(train_cluster0_tf, 7)

clusters_0 = MiniBatchKMeans(n_clusters=3, init_size=1024, batch_size=2048, random_state=9).fit_predict(train_cluster0_tf)

def get_top_keywords(data, clusters, labels, n_terms):
    df = pd.DataFrame(data.todense()).groupby(clusters_0).mean()
    
    for i,r in df.iterrows():
        print('\nCluster {}'.format(i))
        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))
            
get_top_keywords(train_cluster0_tf, clusters_0, cluster0_tf.get_feature_names(), 100)

#CLUSTER 0 == CLUSTER FOR DEMOCRATS
grupos_features_c0 = {
    'CLUSTER0':['we','new','for the','public','use','when','also','and the','elect','accord to','are','by the','secur','into','accord','after','would','hous','one','on the','relea','democrat','clinton', 'foundat','news','but','more','committ','intellig','server','or','document','time','they','russia','secretari','govern','director','been','obama','secretari','state','who','state','state depart','trump','foundat','at','presid','comey','offici','secretari','his','be','has','russian','campaign','said','inform','report','former','from','depart','is','it','state','fbi','clinton','hillari', 'clinton','for','on','was','investig','hillari','fbi','email','clinton']}

grupos_features0 = pd.DataFrame(grupos_features_c0)
grupos_features0['CLUSTER0'].value_counts()

plt.figure(figsize=(15,10))
sns.countplot(y='CLUSTER0',data=grupos_features0)
plt.title('IMPORTANT FEATURES FOR CLUSTER 0 "DEMOCRATS"')

grupos_features1={'CLUSTER1':['take','news','know','want','them','new','over','here','other','call','clinton','democrat','re','him','america','support','get','donald trump','make','report','how','time','donald','no','go','there','after','had','were','hillari','do','up','state','year','been','just','or','trump','can','say','would','so','american','if','like','more','all','when','donald','one','our','obama','out','people','she','presid','in','by','they','not','trump','he','it','republican']}

grupos_features1 =pd.DataFrame(grupos_features1)

plt.figure(figsize=(15,10))
sns.countplot(y='CLUSTER1',data=grupos_features1)
plt.title('IMPORTANT FEATURES FOR CLUSTER 1 "Neutral"')
plt.figure(figsize=(12,8))

grupos_features2={
    'CLUSTER2':['one','polic','white','report','time','an','clinton','fox','august','like','march','crowd','status','our','up','black','just','people','after','not','donald', 'trump','when','tweet','decemb','from','news','september','get','as','ralli','realdonaldtrump','donald','have','about','what','out','januari','she','httpstwittercom','httpstwitter','octob','trump',' support','presid', 'trump','cnn','support','novemb','hillari','her','here','he','be','video','his','with','protest','was','presid','at','you','that','co','httpsco','youtub','httpswwwyoutub','httpswww','www,youtubcom','youtubcom', 'watch','wwwyoutub','comwatch','wwwyoutubcom','watch','to','trump','https','pic','twitter','twitter','twitter','twitter','com']}

#Republicans Dataset
grupos_features2 = pd.DataFrame(grupos_features2)

plt.figure(figsize=(15,10))
sns.countplot(y='CLUSTER2',data=grupos_features2)
plt.title('IMPORTANT FEATURES FOR CLUSTER 2 "Republicans"')

#Creating a new dataframe
limpio.to_csv(r'/gdrive/My Drive/TFM/cluster_data.csv', index = False)

"""## **C) SIR MODEL**

---

# Models as State Transitions
As a quick recap, take a look at the variables we defined:

**N**: total population

**S(t)**: number of people susceptible on day t

**I(t)**: number of people infected on day t

**R(t)**: number of people recovered on day t

**β**: expected amount of people an infected person infects per day

**D**: number of days an infected person has and can spread the disease


**γ**: the proportion of infected recovering per day (γ = 1/D)

**R₀**: the total number of people an infected person infects (R₀ = β / γ)
And here are the basic equations again
"""

#Fake News Dataset
cldt= "/gdrive/My Drive/TFM/clustered_datatype.xlsx"

clustered_datatype = pd.read_excel(cldt)
clustered_datatype.head()

sns.countplot(y='PoliticalBias',data=clustered_datatype)
plt.title('RUMORS POLITICAL BIAS')

color=['Blue','Orange','Red']
plot = clustered_datatype['PoliticalBias'].value_counts().plot.pie(subplots=False, figsize=(7, 4),autopct='%1.2f%%')

#Creating a Dataframe to separte them afterwards
result= clustered_datatype

"""## **GLOBAL EXPLORATORY ANALYSIS**"""

result.info()

result.columns = ['date','review','Rumors','new_clean','Subjectivity','Polarity','Sentiment','clean','Kmeans_cluster','PoliticalBias']
result = result.sort_values(['date'])
result_porfecha = result.groupby('date').sum()
result_porfecha['Confirmed'] = result_porfecha['Rumors'].cumsum()

sns.countplot(y='Rumors',data=result_porfecha)
plt.title('GLOBAL COUNT OF RUMOR SPREAD GROUPED BY DATE')

result_porfecha.describe()

result_tendencia = result_porfecha.drop(columns=['Subjectivity','Polarity','Sentiment','Kmeans_cluster'])
result_tendencia_2 =result_tendencia.drop(columns=['Confirmed'])
result_tendencia = result_tendencia.drop(columns=['Rumors'])

plt.figure(figsize=(10,6))
sns.lineplot(x=result_tendencia.index,y='Confirmed',data=result_tendencia,color='black')
plt.title('REPUBLICANS RUMORS CONFIRMED CASES OVER TIME')
plt.ylabel('NUMBER OF CASES')
plt.xlabel('DATE')

#Creating a dataframe for each political bias
democratas = result[result['PoliticalBias']=='Democrat']
republicanos = result[result['PoliticalBias']=='Republican']
neutrales = result[result['PoliticalBias']=='Neutral']

"""## **DEMOCRATS**"""

democratas.columns = ['date','review','Rumors','new_clean','Subjectivity','Polarity','Sentiment','clean','Kmeans_cluster','PoliticalBias']
democratas = democratas.sort_values(['date'])
democratas_porfecha = democratas.groupby('date').sum()
democratas_porfecha['Confirmed'] = democratas_porfecha['Rumors'].cumsum()

sns.countplot(y='Rumors',data=democratas_porfecha)
plt.title('COUNT OF RUMOR SPREAD FOR DEMOCRATS GROUPED BY DATE')

#GROUPING BY DATE DEMOCRAT RUMORS
democratas_tendencia = democratas_porfecha.drop(columns=['Subjectivity','Polarity','Sentiment','Kmeans_cluster'])
democratas_tendencia_2 = democratas_tendencia.drop(columns=['Confirmed'])
democratas_tendencia = democratas_tendencia.drop(columns=['Rumors'])

plt.figure(figsize=(10,6))
sns.lineplot(x=democratas_tendencia.index,y='Confirmed',data=democratas_tendencia,color='green')
plt.title('DEMOCRATS RUMORS CONFIRMED CASES OVER TIME')
plt.ylabel('NUMBER OF CASES')
plt.xlabel('DATE')

"""## **NEUTRAL**"""

neutrales.columns = ['date','review','Rumors','new_clean','Subjectivity','Polarity','Sentiment','clean','Kmeans_cluster','PoliticalBias']
neutrales = neutrales.sort_values(['date'])
neutrales_porfecha = neutrales.groupby('date').sum()

neutrales_porfecha['Confirmed'] = neutrales_porfecha['Rumors'].cumsum()

sns.countplot(y='Rumors',data=neutrales_porfecha)
plt.title('COUNT OF RUMOR SPREAD FOR NEUTRAL BIAS GROUPED BY DATE')

#Creandp dataset para visualizar tendencia de rumores por fecha
neutrales_tendencia = neutrales_porfecha.drop(columns=['Subjectivity','Polarity','Sentiment','Kmeans_cluster'])
neutrales_tendencia_2 = neutrales_tendencia.drop(columns=['Confirmed'])
neutrales_tendencia= neutrales_tendencia.drop(columns=['Rumors'])

plt.figure(figsize=(10,6))
sns.lineplot(x=neutrales_tendencia.index,y='Confirmed',data=neutrales_tendencia, color='orange')
plt.title('NEUTRAL RUMORS CONFIRMED CASES OVER TIME')
plt.ylabel('NUMBER OF CASES')
plt.xlabel('DATE')

"""## **REPUBLICANS**"""

republicanos = republicanos.sort_values(['date'])
republicanos.columns = ['date','review','Rumors','new_clean','Subjectivity','Polarity','Sentiment','clean','Kmeans_cluster','PoliticalBias']
republicanos_porfecha = republicanos.groupby('date').sum()
republicanos_porfecha['Confirmed'] = republicanos_porfecha['Rumors'].cumsum()

sns.countplot(y='Rumors',data=republicanos_porfecha)
plt.title('COUNT OF RUMOR SPREAD FOR REPUBLICANS GROUPED BY DATE')

republicans_tendencia = republicanos_porfecha.drop(columns=['Subjectivity','Polarity','Sentiment','Kmeans_cluster'])
republicans_tendencia_2= republicans_tendencia.drop(columns=['Confirmed'])
republicans_tendencia = republicans_tendencia.drop(columns=['Rumors'])

plt.figure(figsize=(10,6))
sns.lineplot(x=republicans_tendencia.index,y='Confirmed',data=republicans_tendencia)
plt.title('REPUBLICANS RUMORS CONFIRMED CASES OVER TIME')
plt.ylabel('NUMBER OF CASES')
plt.xlabel('DATE')

plt.figure(figsize=(12,10))
plt.subplot(2, 2, 1)
result_tendencia.plot(ax=plt.gca(), title='GLOBAL RUMORS CONFIRMED CASES OVER TIME',color='green')
plt.ylabel("CONFIRMED CASES",size=13)


plt.subplot(2, 2, 2)
result_tendencia_2.plot(ax=plt.gca(), title='GLOBAL RUMORS BEHAVIOR OVER TIME',color='green')
plt.ylabel('RUMOR BEHAVIOR')

plt.figure(figsize=(12,10))
plt.subplot(2, 2, 1)
democratas_tendencia.plot(ax=plt.gca(), title='DEMOCRATS RUMORS CONFIRMED CASES OVER TIME',color='blue')
plt.ylabel("CONFIRMED CASES")


plt.subplot(2, 2, 2)
democratas_tendencia_2.plot(ax=plt.gca(), title='DEMOCRATS RUMORS BEHAVIOR OVER TIME',color='darkblue')
plt.ylabel("RUMOR BEHAVIOR")

plt.figure(figsize=(12,10))
plt.subplot(2, 2, 1)
neutrales_tendencia.plot(ax=plt.gca(), title='NEUTRAL RUMORS CONFIRMED CASES OVER TIME',color='gray')
plt.ylabel("CONFIRMED CASES")


plt.subplot(2, 2, 2)
neutrales_tendencia_2.plot(ax=plt.gca(), title='NEUTRAL RUMORS BEHAVIOR OVER TIME',color='gray')
plt.ylabel("RUMOR BEHAVIOR")



plt.figure(figsize=(12,10))
plt.subplot(2, 2, 1)
republicans_tendencia.plot(ax=plt.gca(), title='REPUBLICANS RUMORS CONFIRMED CASES OVER TIME',color='red')
plt.ylabel("CONFIRMED CASES")


plt.subplot(2, 2, 2)
republicans_tendencia_2.plot(ax=plt.gca(), title='REPUBLICANS RUMORS BEHAVIOR OVER TIME',color='red')
plt.ylabel("RUMOR BEHAVIOR")

"""## **SIR FOR GLOBAL POPULATION**"""

import matplotlib.pyplot as plt
import numpy as np

ndays = 200
dt = 1 #time step in days
beta = 0.14#infection rate
gamma = 0.1
S = np.zeros(ndays)#susceptible
I = np.zeros(ndays)#infected
R = np.zeros(ndays)#recovered
t =np.arange(ndays)*dt
r0 = beta/gamma
I[0] = 0.01 #initial infective proportion
S[0] = 1 - I[0] #initial susceptible
R[0] = 0.

for i in range (ndays-1):
  S[i+1] = S[i] - beta*(S[i]*I[i])*dt
  I[i+1] = I[i] + (beta*S[i]*I[i]-gamma*I[i])*dt
  R[i+1] = R[i] + (gamma*I[i])*dt

fig = plt.figure(1); fig.clf()
plt.plot(t,S,'b',lw=3, label='Susceptible')
plt.plot(t,I,'r',lw=3, label='Infective')
plt.plot(t,R,'g',lw=3, label='Removed')

fig.legend();plt.xlabel('Days'); plt.ylabel('Fraction of population');plt.title('GLOBAL SIR MODEL')

"""## **SIR FOR DEMOCRATS**"""

import matplotlib.pyplot as plt
import numpy as np

ndays = 200
dt = 1 #time step in days
beta = 0.23#infection rate
gamma = 0.1
S = np.zeros(ndays)#susceptible
I = np.zeros(ndays)#infected
R = np.zeros(ndays)#recovered
t =np.arange(ndays)*dt
r0 = beta/gamma
I[0] = 0.01 #initial infective proportion
S[0] = 1 - I[0] #initial susceptible
R[0] = 0.

for i in range (ndays-1):
  S[i+1] = S[i] - beta*(S[i]*I[i])*dt
  I[i+1] = I[i] + (beta*S[i]*I[i]-gamma*I[i])*dt
  R[i+1] = R[i] + (gamma*I[i])*dt

fig = plt.figure(1); fig.clf()
plt.plot(t,S,'b',lw=3, label='Susceptible')
plt.plot(t,I,'r',lw=3, label='Infective')
plt.plot(t,R,'g',lw=3, label='Removed')

fig.legend();plt.xlabel('Days'); plt.ylabel('Fraction of population');plt.title('SIR MODEL FOR DEMOCRATS')

"""## **SIR FOR NEUTRALS**"""

import matplotlib.pyplot as plt
import numpy as np

ndays = 200
dt = 1 #time step in days
beta = 0.40#infection rate
gamma = 0.1
S = np.zeros(ndays)#susceptible
I = np.zeros(ndays)#infected
R = np.zeros(ndays)#recovered
t =np.arange(ndays)*dt
r0 = beta/gamma
I[0] = 0.01 #initial infective proportion
S[0] = 1 - I[0] #initial susceptible
R[0] = 0.

for i in range (ndays-1):
  S[i+1] = S[i] - beta*(S[i]*I[i])*dt
  I[i+1] = I[i] + (beta*S[i]*I[i]-gamma*I[i])*dt
  R[i+1] = R[i] + (gamma*I[i])*dt

fig = plt.figure(1); fig.clf()
plt.plot(t,S,'b',lw=3, label='Susceptible')
plt.plot(t,I,'r',lw=3, label='Infective')
plt.plot(t,R,'g',lw=3, label='Removed')

fig.legend();plt.xlabel('Days'); plt.ylabel('Fraction of population');plt.title('SIR MODEL FOR NEUTRAL POP')

"""## **SIR FOR REPUBLICANS**"""

import matplotlib.pyplot as plt
import numpy as np

ndays = 200
dt = 1 #time step in days
beta = 0.54#infection rate
gamma = 0.1
S = np.zeros(ndays)#susceptible
I = np.zeros(ndays)#infected
R = np.zeros(ndays)#recovered
t =np.arange(ndays)*dt
r0 = beta/gamma
I[0] = 0.01 #initial infective proportion
S[0] = 1 - I[0] #initial susceptible
R[0] = 0.

for i in range (ndays-1):
  S[i+1] = S[i] - beta*(S[i]*I[i])*dt
  I[i+1] = I[i] + (beta*S[i]*I[i]-gamma*I[i])*dt
  R[i+1] = R[i] + (gamma*I[i])*dt

fig = plt.figure(1); fig.clf()
plt.plot(t,S,'b',lw=3, label='Susceptible')
plt.plot(t,I,'r',lw=3, label='Infective')
plt.plot(t,R,'g',lw=3, label='Removed')

fig.legend();plt.xlabel('Days'); plt.ylabel('Fraction of population');plt.title('SIR MODEL FOR REPUBLICANS')